title: understanding Cross Entrophy
date: 2018-12-22 21:04:10
tags:
math: true
---

在我们使用Softmax将特征向量映射成概率值之后，我们需要一个指标来评价预测出来的
概率值的好坏，通常，我们使用的就是cross-entrophy loss，即交叉熵损失。交叉熵损
失的计算公式是：$ L = -(ylog\hat{y} + (1-y)log(1-\hat{y})) $。其中， $\hat{y}$ 表示预测出的概率值，而 $y$ 表示实际的分类标签值。


## 直观理解
我们已知的是现在真实的标签，假设这就是整体数据的分布，那么我们要让这个分布的
概率最大，也就是说，当标签是 $y_i$ 时，我们让预测值 $\hat{y}$ 最大，可以表示成 $\hat{y}^{y_i}$ ，将每种可能的 $y_i$ 情况相乘，最终得到 $\hat{y}^{y}*(1-\hat{y})^{1-y}$ 这样的形式，然后对这个函数求$log$，最大化这个数 $L_{tmp}$ ，就是最小化 $-L_{tmp}=L=-(ylog\hat{y}+ (1-y)log(1-\hat{y})$，其实这就是极大似然的原理和推导过程。

但是为什么这个被叫做交叉熵损失呢？我们再尝试从熵的角度去理解这个损失所代表的
含义。

### 信息熵

在理解交叉熵之前，我们先复习一下什么是熵。熵是香农信息论里的概念，表示的是信
息的复杂度，或者信息量的大小。我们通常说的信息量大，其实就是说这件事的不确定
性更大，平时发生的概率小，比如说刘恺威和杨幂离婚了！一个大瓜的那种感觉。

香农把这种感觉用了定量的方式去定义，我们传递信息需要用语言，用文字，如果认为
有个码本去传递信息，那么熵就是在特定的概率分布 $p$ 下，传递所有事件所需要的最
小的平均码长。平均码长越长，说明分布下的事件越复杂。比如，平均码长是0，那说
明没有信息需要传递，事件发生的概率是1。

信息熵可以表示成 $H(p)=\sum_{x}p(x)*len(x)$ ，即 $x$ 发生的概率乘以它的编码长度，因为是最小的平均编码，那么需要当事件发生的概率大的时候，它的编码长度更小，这样总体的编码长度就小。这是一个直观想法，那怎么计算具体的编码长度 $len(x)$ 呢？

当我们进行变长编码的时候，我们需要保证每个编码是唯一的，且不能是其他编码的前
缀，否则会出现解析的问题，比如二进制编码中，如果把1作为一个事件的编码，那么10,
11, 100, 101, ..., 全部不可以作为其他时间的编码。就像是，我们为了一颗树，放弃了这颗树背后的森林。假设编码长度是$L$，那么放弃的就是$\int_{L}^{\infty} \frac{1}{2^L} dL$ ，求出来是$\frac{1}{2^L}ln2$ ， 如果是以$e$为底，则就是$\frac{1}{2^L}$。因为当 $p(x)$ 越大的时候，我们越愿意放弃更多的森林来让总体的码长更短，因此另 $p(x)=\frac{1}{2^L}$，即码长 $L=log_{2}{\frac{1}{p(x)}}$。
所以信息熵为：
$ H(p)=\sum_xp(x)*log_2(\frac{1}{p(x)})$ 。

### 交叉熵

如果分布p和分布q传递相同的一些时间，但是事件发生的概率不同，在分布q中，我们依然用刚才根据分布p得出的码本，那么总的编码长度就是:
$ H_{p}(q) = \sum_x{q(x) \log_2{\frac{1}{p(x)}} } $
，即交叉熵，这个熵值越大，说明分布p和
分布q的差距越大。回到我们最初的场景，我们知道真实标签$y$ 和计算出的分类概率
$\hat{y}$，于是我们计算出在真实分布的情况下预测出的信息熵，即:
$\sum_y{\log(\frac{1}{\hat{y}}) + (1-y) \log( \frac{1}{1-\hat{y}}) }$。
我们要最小化这个信息熵，也就是使预测出来的概率分布接近于真实的概率分布。


### Reference

[1] http://colah.github.io/posts/2015-09-Visual-Information/ 
